{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Processing Scraped Data"
      ],
      "metadata": {
        "id": "MYEHR17OP7Ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "zF5vUeU3RUMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnOzVIWoP4uM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "\n",
        "\n",
        "# Path to input CSV file\n",
        "input_csv_path = ''  # Replaced with csv outputs from WebScraper for each store\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(input_csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Cleaning of Raw Scraped Data"
      ],
      "metadata": {
        "id": "LR4DagOZTCmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Regex pattern to match price\n",
        "price_pattern = re.compile(r'\\$\\d+\\.\\d+|Priced by add-ons')\n",
        "\n",
        "# Initialize lists to hold item names and extracted prices\n",
        "item_names = []\n",
        "extracted_prices = []\n",
        "\n",
        "current_item_name = None\n",
        "current_item_price = None\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    # Check if the row could belong to a new item based on the 'Name.text' column\n",
        "    if pd.notna(row['Name.text']) and row['Name.text'].strip():\n",
        "        if current_item_name is not None:\n",
        "            item_names.append(current_item_name)\n",
        "            extracted_prices.append(current_item_price)\n",
        "\n",
        "        current_item_name = row['Name.text']\n",
        "        current_item_price = \"No price info\"\n",
        "\n",
        "    if pd.notna(row['Price.text']) and str(row['Price.text']).strip():\n",
        "        text_to_search = str(row['Price.text'])\n",
        "        price_match = price_pattern.search(text_to_search)\n",
        "        if price_match:\n",
        "            current_item_price = price_match.group()\n",
        "\n",
        "# Append the last item's details\n",
        "if current_item_name is not None:\n",
        "    item_names.append(current_item_name)\n",
        "    extracted_prices.append(current_item_price)\n",
        "\n",
        "# Create a new DataFrame with item names and their extracted prices\n",
        "df_extract = pd.DataFrame({\n",
        "    'Name': item_names,\n",
        "    'Price': extracted_prices\n",
        "})\n",
        "\n",
        "\n",
        "# Filter out rows where 'Price' is \"No price info\"\n",
        "cleaned_df = extract_df[extract_df['Price'] != \"No price info\"]\n",
        "\n",
        "# Filter out rows where Price is 0 and \"Name\" column is not empty\n",
        "df_filtered = cleaned_df[(cleaned_df['Price'] != 0) & (df['Name'].notna())]\n",
        "\n",
        "# Drop duplicate rows based on the \"Name\" column\n",
        "cleaned_df = df_filtered.drop_duplicates(subset=['Name'], keep='first')\n",
        "\n",
        "# Save to CSV\n",
        "output_csv_path = '/content/output.csv'\n",
        "output_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "output_csv_path\n",
        "\n"
      ],
      "metadata": {
        "id": "nBmmEsOYR2FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Non-Food Items"
      ],
      "metadata": {
        "id": "ZnPkcyiZUVfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to input CSV\n",
        "input_csv_path = ''  # Per store\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(input_csv_path)\n",
        "\n",
        "# List of keywords to filter out\n",
        "keywords = [\n",
        "    'nail polish',\n",
        "    'probiotics',\n",
        "    \"nature's way\",\n",
        "    'mineral fusion',\n",
        "    'tints of nature',\n",
        "    'Tissue',\n",
        "    'Floral',\n",
        "    'Water',\n",
        "    'pads',\n",
        "    'collagen',\n",
        "    'kitchen',\n",
        "    'bath',\n",
        "    'garbage',\n",
        "    'dishwasher',\n",
        "    'diapers',\n",
        "    'detergent',\n",
        "    'air freshener',\n",
        "    'wipes',\n",
        "    'barbie',\n",
        "    'doll',\n",
        "    'craft',\n",
        "    'odor',\n",
        "    'laundry',\n",
        "    'sponges',\n",
        "    'gloves',\n",
        "    'scented oil',\n",
        "    'lint roller',\n",
        "    'lotion',\n",
        "    'tissue',\n",
        "    'knife',\n",
        "    'knives',\n",
        "    'puppy',\n",
        "    'dog',\n",
        "    'cat',\n",
        "    'clean',\n",
        "    'pet',\n",
        "    'refill',\n",
        "    'dryer',\n",
        "    'heavy duty',\n",
        "    'soap',\n",
        "    'sanitizer',\n",
        "    'dish soap',\n",
        "    'vacuum',\n",
        "    'insect trap',\n",
        "    'play set',\n",
        "    'playset',\n",
        "    'cotton',\n",
        "    'dishwasher',\n",
        "    'cleaning',\n",
        "    'cloth',\n",
        "    'sleep',\n",
        "    'repellent',\n",
        "    'robot',\n",
        "    'laptop',\n",
        "    'cleaner',\n",
        "    'contraceptive',\n",
        "    'hardwood',\n",
        "    'scent',\n",
        "    'pan',\n",
        "    'multi surface',\n",
        "    'surface',\n",
        "    'pad',\n",
        "    'dusting',\n",
        "    'kit',\n",
        "    'duster',\n",
        "    'iRobot',\n",
        "    'hefty',\n",
        "    'plastic',\n",
        "    'dish',\n",
        "    'spoon',\n",
        "    'dryer',\n",
        "    'containers',\n",
        "    'cat litter',\n",
        "    'gadget',\n",
        "    'toilet',\n",
        "    'infusion',\n",
        "    'fabric',\n",
        "    'covid test',\n",
        "    'cat litter',\n",
        "    'bath tissue',\n",
        "    'aluminum',\n",
        "    'toilet',\n",
        "    'laundry',\n",
        "    'medicine',\n",
        "    'cold remedy',\n",
        "    'congestion',\n",
        "    'water',\n",
        "    'cold_remedy',\n",
        "    'flu',\n",
        "    'paper',\n",
        "    'cough',\n",
        "    'mucus',\n",
        "    'Robitussin',\n",
        "    'mucinex',\n",
        "    'cold & flu'\n",
        "]\n",
        "\n",
        "# Normalize keywords for case-insensitive matching\n",
        "normalized_keywords = [keyword.lower() for keyword in keywords]\n",
        "\n",
        "# Function to determine if a row should be excluded based on keywords\n",
        "def is_excluded(item_name):\n",
        "    if isinstance(item_name, str):\n",
        "        item_name_lower = item_name.lower()\n",
        "        for keyword in normalized_keywords:\n",
        "            if keyword in item_name_lower:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Filter the DataFrame\n",
        "cleaned_df = df[~df['Name'].apply(is_excluded)]\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "cleaned_csv_path = '' # Updated per store\n",
        "cleaned_df.to_csv(cleaned_csv_path, index=False)\n",
        "\n",
        "print(f\"Cleaned data saved to {cleaned_csv_path}\")"
      ],
      "metadata": {
        "id": "KUv_MoZyTGnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Census API & Calculate Initial $NRF_{9.3}$ Values For All Items"
      ],
      "metadata": {
        "id": "VhsqHQAuUmlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_percent_dv(nutrient_value, nutrient_id):\n",
        "    # RDIs for calculating %DV, expressed in the units provided by the API\n",
        "    rdi_values = {\n",
        "        '1003': 50,       # Protein in g\n",
        "        '1079': 25,       # Fiber in g\n",
        "        '1106': 900,      # Vitamin A in Î¼g\n",
        "        '1162': 90,       # Vitamin C in mg\n",
        "        '1109': 15,       # Vitamin E in mg\n",
        "        '1087': 1300,     # Calcium in mg\n",
        "        '1089': 18,       # Iron in mg\n",
        "        '1090': 400,      # Magnesium in mg\n",
        "        '1092': 4700,     # Potassium in mg\n",
        "        '1258': 20,       # Saturated Fat in g (limit)\n",
        "        '2000': 50,       # Added Sugars in g (limit)\n",
        "        '1093': 2300,     # Sodium in mg (limit)\n",
        "    }\n",
        "    return (nutrient_value / rdi_values.get(nutrient_id, 1)) * 100\n",
        "\n",
        "def calculate_nrf93(food_nutrients):\n",
        "    positive_nutrients = ['1003', '1079', '1106', '1162', '1109', '1087', '1089', '1090', '1092']\n",
        "    negative_nutrients = ['1258', '2000', '1093']\n",
        "    positive_sum = sum(calculate_percent_dv(n['value'], str(n['nutrientId'])) for n in food_nutrients if str(n['nutrientId']) in positive_nutrients)\n",
        "    negative_sum = sum(calculate_percent_dv(n['value'], str(n['nutrientId'])) for n in food_nutrients if str(n['nutrientId']) in negative_nutrients)\n",
        "    return positive_sum - negative_sum\n",
        "\n",
        "def get_nrf93_score(food_name):\n",
        "    API_KEY = '6ctPProwthJHdBqi73oO5ZO6CWLTHXGW616X90f9'\n",
        "    BASE_URL = 'https://api.nal.usda.gov/fdc/v1/foods/search'\n",
        "    params = {\n",
        "        'query': food_name,\n",
        "        'api_key': API_KEY,\n",
        "        'dataType': ['Foundation', 'SR Legacy'],\n",
        "        'pageSize': 2\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "    scores = [calculate_nrf93(food['foodNutrients']) for food in data.get('foods', [])]\n",
        "    return sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "# # Load the CSV file\n",
        "df = pd.read_csv('/content/fast_food_pre_nrf.csv') # Completed by store type at first\n",
        "\n",
        "# # Calculate the NRF9.3 score for each product name and store in a new column\n",
        "df['NRF'] = df['Name'].apply(get_nrf93_score)\n",
        "\n",
        "# # Save the updated DataFrame to a new CSV file\n",
        "df.to_csv('fast_food_post_nrf_REAL.csv', index=False)"
      ],
      "metadata": {
        "id": "xHV5ACi1TdZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Associating Census Tracts to Stores With Google Maps API"
      ],
      "metadata": {
        "id": "F7WRHtpLRIxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "E0rUxKJCVNS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpledbf"
      ],
      "metadata": {
        "id": "tlxZOwknVPMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import string\n",
        "import requests\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from simpledbf import Dbf5\n",
        "from shapely.geometry import Point, shape\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "M9RKMf3tVVBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Census Tract Geographical Data Files & Visualize"
      ],
      "metadata": {
        "id": "v8EqylTIVhB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
        "\n",
        "# Read Shapefile\n",
        "gdf = gpd.read_file('/content/sample_data/tl_2023_34_tract.shp')\n",
        "\n",
        "# Display the DataFrame structure with non-geometry columns\n",
        "dbf = Dbf5('/content/sample_data/tl_2023_34_tract.dbf')\n",
        "df = dbf.to_dataframe()\n",
        "\n",
        "# Merge DataFrame with your GeoDataFrame based on the index\n",
        "gdf = gdf.merge(df, left_index=True, right_index=True)\n",
        "\n",
        "gdf.plot()\n",
        "\n",
        "# Mercer County Code = 021\n",
        "mercer_county_fips_code = '021'\n",
        "\n",
        "# Filter the GeoDataFrame to only include tracts from the specified county\n",
        "filtered_gdf = gdf[gdf['COUNTYFP_x'] == mercer_county_fips_code]\n",
        "\n",
        "# Test to confirm shape and info\n",
        "filtered_gdf.info()\n",
        "print(filtered_gdf.head())\n",
        "filtered_gdf.plot()"
      ],
      "metadata": {
        "id": "fXoys_q2Vfws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Google Maps API for Stores, Create JSON Before Adding FRCS Values"
      ],
      "metadata": {
        "id": "XDOZujQrWCKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query each tract around 5km radius except the 5 outliers to create stores_jason"
      ],
      "metadata": {
        "id": "UVB3mLZ2WRbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Google Maps API key and store types\n",
        "api_key = ''\n",
        "store_types = ['grocery_or_supermarket', 'convenience_store', 'meal_takeaway']\n",
        "\n",
        "# Hardcoded radii for specific tracts over 5000m in radius\n",
        "specific_radii = {\n",
        "    '34021003905': 5665,\n",
        "    '34021003001': 5039,\n",
        "    '34021003800': 8497,\n",
        "    '34021003304': 5153,\n",
        "    '34021004204': 6050,\n",
        "}\n",
        "\n",
        "all_stores_data = []\n",
        "\n",
        "# API Request\n",
        "def make_request(location, store_type, radius, api_key):\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "    params = {\n",
        "        'location': location,\n",
        "        'radius': str(radius),  # Dynamic radius\n",
        "        'type': store_type,\n",
        "        'key': api_key\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get('results', [])\n",
        "    else:\n",
        "        print(f\"API request failed for store type {store_type} with response: {response.text}\")\n",
        "        return []\n",
        "\n",
        "# Query stores for each tract, around its geometry, based on store types\n",
        "def find_stores_by_types(tract_id, tract_geometry, store_types, api_key):\n",
        "    stores_found = []\n",
        "    lat, lon = tract_geometry.centroid.y, tract_geometry.centroid.x\n",
        "    location = f\"{lat},{lon}\"\n",
        "\n",
        "    # Determine the radius to use\n",
        "    radius = specific_radii.get(tract_id, 5000)  # Use specific radius if available, else default to 5000\n",
        "\n",
        "    for store_type in store_types:\n",
        "        results = make_request(location, store_type, radius, api_key)\n",
        "        for result in results:\n",
        "            store_name = result['name']\n",
        "            stores_found.append({\n",
        "                'name': store_name,\n",
        "                'type': store_type,\n",
        "            })\n",
        "    return stores_found\n",
        "\n",
        "\n",
        "query_gdf = gdf[gdf['COUNTYFP_x'] == mercer_county_fips_code]\n",
        "\n",
        "# Loop through all\n",
        "for index, row in query_gdf.iterrows():\n",
        "    tract_id = row['GEOID_x']\n",
        "    tract_stores = find_stores_by_types(tract_id, row['geometry'], store_types, api_key)\n",
        "\n",
        "    # Structure to store current tract's store information in json\n",
        "    tract_data = {\n",
        "        \"CensusTractID\": tract_id,\n",
        "        \"StoreCount\": len(tract_stores),\n",
        "        \"Stores\": tract_stores\n",
        "    }\n",
        "    all_stores_data.append(tract_data)\n",
        "\n",
        "# Convert to JSON\n",
        "stores_json = json.dumps(all_stores_data, indent=4)\n",
        "print(stores_json)"
      ],
      "metadata": {
        "id": "p5LDgg6JWCpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Util Code for Finalizing JSON With Stores per Tract"
      ],
      "metadata": {
        "id": "KJny6Z77XaGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize data from stores_json to see what stores are missing or there by mistake"
      ],
      "metadata": {
        "id": "IyvRzLzWXiIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON file\n",
        "with open('/content/completed_mercer_county_tract_retailers.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Lists of observed stores\n",
        "observed_stores = {\n",
        "    \"grocery_or_supermarket\": ['Target', 'Wegmans', 'Shoprite', \"McCaffrey's Food Market\", 'Walmart',\n",
        "                               'ALDI', 'Stop & Shop', 'ACME', 'Costco', \"BJ's Wholesale Club\"],\n",
        "    \"convenience_store\": ['7-Eleven', 'Wawa', 'CVS', 'Walgreens'],\n",
        "    \"meal_takeaway\": [\"McDonald's\", \"Wendy's\", 'Burger King', 'Taco Bell', \"Domino's\"]\n",
        "}\n",
        "\n",
        "# Initialize dictionaries to count occurrences of each store\n",
        "store_counts_observed = {\n",
        "    \"grocery_or_supermarket\": defaultdict(int),\n",
        "    \"convenience_store\": defaultdict(int),\n",
        "    \"meal_takeaway\": defaultdict(int)\n",
        "}\n",
        "\n",
        "store_counts_all = {\n",
        "    \"grocery_or_supermarket\": defaultdict(int),\n",
        "    \"convenience_store\": defaultdict(int),\n",
        "    \"meal_takeaway\": defaultdict(int)\n",
        "}\n",
        "\n",
        "# Function to normalize store names and check for observed store names\n",
        "def normalize_store_name(store_name, category):\n",
        "    for observed_store in observed_stores[category]:\n",
        "        if observed_store.lower() in store_name.lower():\n",
        "            return observed_store\n",
        "    return None\n",
        "\n",
        "# Function to print the store counts as a table\n",
        "def print_store_counts(category, store_counts):\n",
        "    print(f\"\\n{category.replace('_', ' ').title()}\")\n",
        "    for store, count in store_counts[category].items():\n",
        "        print(f\"{store}: {count}\")\n",
        "\n",
        "\n",
        "# Loop through each tract and each store, normalize the store names, and count occurrences\n",
        "for tract in data:\n",
        "    for store in tract[\"Stores\"]:\n",
        "        store_type = store[\"type\"]\n",
        "        normalized_name = normalize_store_name(store[\"name\"], store_type)\n",
        "        if normalized_name:\n",
        "            store_counts_observed[store_type][normalized_name] += 1\n",
        "        else:\n",
        "            store_counts_observed[store_type][\"Unknown\"] += 1\n",
        "        # Count all stores including unobserved as their own entries\n",
        "        store_counts_all[store_type][store[\"name\"]] += 1\n",
        "\n",
        "# Print total and observed counts\n",
        "for category in store_counts_observed.keys():\n",
        "    total_stores = sum(store_counts_all[category].values())\n",
        "    observed_stores = sum(store_counts_observed[category].values()) - store_counts_observed[category][\"Unknown\"]\n",
        "    print(f\"\\nCategory: {category.replace('_', ' ').title()}\")\n",
        "    print(f\"Total Stores: {total_stores}\")\n",
        "    print(f\"Observed Stores: {observed_stores}\")\n",
        "\n",
        "# Print tables where unobserved stores are aggregated under \"Unknown\"\n",
        "print(\"\\nFirst set of tables (Unobserved stores counted as 'Unknown'):\")\n",
        "for category in store_counts_observed.keys():\n",
        "    print_store_counts(category, store_counts_observed)\n",
        "\n",
        "# Print tables listing each unique store name, including unobserved stores\n",
        "print(\"\\nSecond set of tables (All stores listed individually):\")\n",
        "for category in store_counts_all.keys():\n",
        "    print_store_counts(category, store_counts_all)\n"
      ],
      "metadata": {
        "id": "kk4tNvj3Xd_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporarily add centroids for each tract to manually search missing stores with observed data"
      ],
      "metadata": {
        "id": "EMmssSd6X0wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your JSON data\n",
        "with open(\"/content/mercer_county_tract_retailers.json\", \"r\") as f:\n",
        "    tract_data = json.load(f)\n",
        "\n",
        "# Modified loop to add centroid at the beginning of each tract entry\n",
        "for i, tract in enumerate(tract_data):\n",
        "    # Find the corresponding row in query_gdf\n",
        "    tract_gdf_row = query_gdf[query_gdf['GEOID_x'] == tract[\"CensusTractID\"]].iloc[0]\n",
        "\n",
        "    # Calculate the centroid\n",
        "    centroid = tract_gdf_row.geometry.centroid\n",
        "    lat, lon = centroid.y, centroid.x\n",
        "\n",
        "    # Reconstruct the tract entry with centroid at the beginning\n",
        "    new_tract_data = {\n",
        "        \"CensusTractID\": tract[\"CensusTractID\"],\n",
        "        \"centroid\": {\"latitude\": lat, \"longitude\": lon},\n",
        "        \"StoreCount\": tract[\"StoreCount\"],\n",
        "        \"Stores\": tract[\"Stores\"]\n",
        "    }\n",
        "\n",
        "    # Update the original list with the new tract data\n",
        "    tract_data[i] = new_tract_data\n",
        "\n",
        "# Print the modified JSON data\n",
        "print(json.dumps(tract_data, indent=4))\n"
      ],
      "metadata": {
        "id": "AiTx3NslX0hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually search for missing stores by name and add to JSON and export"
      ],
      "metadata": {
        "id": "OliAKOClX2Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load existing data\n",
        "with open(\"/content/temp_centroids_added.json\", \"r\") as f:\n",
        "    tracts_data = json.load(f)\n",
        "\n",
        "# Define the stores to query\n",
        "store_queries = {\n",
        "    \"grocery_or_supermarket\": [\"Stop & Shop\", \"Costco Wholesale\", \"BJs Wholesale Club\", \"McCaffrey's Food Market-Princeton\", \"McCaffrey's Food Market - West Windsor\", \"Walmart\", \"Walmart Supercenter\"],\n",
        "    \"meal_takeaway\": [\"McDonald\", \"Wendy\", \"Burger King\"]\n",
        "}\n",
        "\n",
        "# Removes punctuation and convert to lowercase\n",
        "def normalize_name(name):\n",
        "    remove_punctuation = str.maketrans('', '', string.punctuation)\n",
        "    return name.lower().translate(remove_punctuation).strip()\n",
        "\n",
        "# Checks for close match\n",
        "def is_close_match(result_name, query_names):\n",
        "    normalized_result_name = normalize_name(result_name)\n",
        "    for query_name in query_names:\n",
        "        normalized_query_name = normalize_name(query_name)\n",
        "        if normalized_query_name in normalized_result_name and \\\n",
        "           len(normalized_result_name) <= len(normalized_query_name) + 2:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Check if the store is already listed and adds if not\n",
        "def add_missing_stores(tract, api_key, store_queries):\n",
        "    added_stores = []\n",
        "    for category, store_names in store_queries.items():\n",
        "        for store_name in store_names:\n",
        "            location = f\"{tract['centroid']['latitude']},{tract['centroid']['longitude']}\"\n",
        "            radius = \"5000\"\n",
        "            url = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "            params = {\n",
        "                \"location\": location,\n",
        "                \"radius\": radius,\n",
        "                \"keyword\": store_name,\n",
        "                \"key\": api_key\n",
        "            }\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                results = response.json().get(\"results\", [])\n",
        "                for result in results:\n",
        "                    if is_close_match(result[\"name\"], [store_name]):\n",
        "                        store_data = {\n",
        "                            \"name\": result[\"name\"],\n",
        "                            \"type\": category\n",
        "                        }\n",
        "                        if store_data not in tract[\"Stores\"]:\n",
        "                            tract[\"Stores\"].append(store_data)\n",
        "                            added_stores.append(store_data)\n",
        "                            print(f\"Added {store_data['name']} to Tract ID {tract['CensusTractID']}\")\n",
        "    return added_stores\n",
        "\n",
        "\n",
        "# Iterate over each tract and add missing stores\n",
        "for tract in tracts_data:\n",
        "    add_missing_stores(tract, api_key, store_queries)\n",
        "\n",
        "# Save the updated data back to JSON\n",
        "with open(\"completed_mercer_county_tract_retailers.json\", \"w\") as f:\n",
        "    json.dump(tracts_data, f, indent=4)\n",
        "\n",
        "print(\"Updated JSON saved.\")\n"
      ],
      "metadata": {
        "id": "zssHEO6jX8_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Clustering and Predicting Labels"
      ],
      "metadata": {
        "id": "MkhMiWOARD42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and initial checks"
      ],
      "metadata": {
        "id": "F3xkFtQqZNOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial checks include printing figure that shows Average FRCS by category vs $\\Theta$"
      ],
      "metadata": {
        "id": "bcwRrp7GZO8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import fsolve\n",
        "from sklearn.cluster import KMeans\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import silhouette_score\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "# Hardcoded data\n",
        "avg_nrf_all = 15.94365475\n",
        "max_nrf_all = 74.41707905\n",
        "min_nrf_all = -24.7830702\n",
        "avg_cost_all = 5.602255578\n",
        "max_cost_all = 12.1995\n",
        "min_cost_all = 1.65\n",
        "\n",
        "# Store hardcoded data values for stores\n",
        "stores_data_dict = {\n",
        "    'Store': ['McCaffrey', 'ACME', 'ALDI', 'BJ', 'Costco', 'ShopRite', 'Stop & Shop', 'Target', 'Walmart', 'Wegman',\n",
        "              'Wawa', 'CVS', '7-Eleven', 'Walgreens',\n",
        "              \"Domino's\", 'Taco Bell', 'McDonald', 'Wendy', 'Burger King'],\n",
        "    'Category': ['grocery_or_supermarket']*10 + ['convenience_store']*4 + ['meal_takeaway']*5,\n",
        "    'Avg NRF': [20.2801, 16.8792, 19.2794, 18.8869, 25.1476, 12.1907, 20.3024, 13.7764, 15.3831, 22.7182,\n",
        "                9.8079, -7.2559, -8.8167, 2.0326,\n",
        "                -7.4033, -5.2312, -11.1748, -6.9643, -11.8451],\n",
        "    'Avg Cost': [6.7002, 5.4859, 3.7146, 6.2001, 6.6022, 5.2021, 6.4960, 5.4342, 4.1983, 6.3191,\n",
        "                 4.7611, 6.3314, 4.3121, 5.3588,\n",
        "                 10.9794, 7.2684, 6.4656, 6.1878, 5.5602],\n",
        "}\n",
        "\n",
        "# Convert to Dataframe\n",
        "stores_data = pd.DataFrame(stores_data_dict)\n",
        "\n",
        "\n",
        "# Load JSON data with tract data\n",
        "with open(\"/content/sample_data/Mercer_country_tract_retailers_pre_frcs.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "# Function to calculate FRCS\n",
        "def calculate_frcs(row, theta):\n",
        "    w1 = theta\n",
        "    w2 = 1 - theta\n",
        "    nrf_component = (row['Avg NRF'] - avg_nrf_all) / (max_nrf_all - min_nrf_all)\n",
        "    cost_component = (row['Avg Cost'] - avg_cost_all) / (max_cost_all - min_cost_all)\n",
        "    return w1 * nrf_component - w2 * cost_component\n",
        "\n",
        "# Function to update json with frcs values given theta\n",
        "def update_json_with_frcs(theta, data):\n",
        "    print('starting update_json_with_frcs for theta ', theta)\n",
        "\n",
        "    # Recalculate FRCS scores based on new theta\n",
        "    for index, row in stores_data.iterrows():\n",
        "        stores_data.at[index, 'FRCS'] = calculate_frcs(row, theta)\n",
        "\n",
        "    # Calculate mean and sd FRCS for each Category\n",
        "    category_stats = stores_data.groupby('Category')['FRCS'].agg(['mean', 'std'])\n",
        "    print('category stats ',category_stats)\n",
        "\n",
        "    # Update the JSON data with these FRCS values\n",
        "    index = 0\n",
        "    for tract in data:\n",
        "        frcs_values = []\n",
        "        print(f'Processing tract {index + 1} of {len(data)}')\n",
        "        index = index + 1\n",
        "        for store in tract['Stores']:\n",
        "            store_name = store[\"name\"].lower()\n",
        "            store_assigned = False\n",
        "            for _, store_row in stores_data.iterrows():\n",
        "                if store_row[\"Store\"].lower() in store_name:\n",
        "                    store[\"FRCS\"] = store_row[\"FRCS\"]\n",
        "                    store_assigned = True\n",
        "                    break\n",
        "\n",
        "            if not store_assigned:\n",
        "                category = store[\"type\"]\n",
        "                if category in category_stats.index:\n",
        "                    mean = category_stats.loc[category, 'mean']\n",
        "                    std = category_stats.loc[category, 'std']\n",
        "                    store[\"FRCS\"] = np.random.normal(mean, std)\n",
        "                else:\n",
        "                    print(f\"No category stats for {store['name']} of type {store['type']}, assigning default FRCS\")\n",
        "\n",
        "            frcs_values.append(store[\"FRCS\"])\n",
        "\n",
        "        if frcs_values:\n",
        "            average_frcs = np.average(frcs_values)\n",
        "            tract['average FRCS'] = average_frcs\n",
        "\n",
        "    print('done')\n",
        "\n",
        "# Function to find optimal k by plotting Silhouette scores and WCSS\n",
        "def find_optimal_k_and_plot_3D(data, theta_values):\n",
        "    # Initialize DataFrames to store WCSS and silhouette scores\n",
        "    wcss_df = pd.DataFrame(index=range(2, 11), columns=theta_values)\n",
        "    silhouette_df = pd.DataFrame(index=range(2, 11), columns=theta_values)\n",
        "\n",
        "    for theta in theta_values:\n",
        "        update_json_with_frcs(theta, data)\n",
        "\n",
        "        # Extract 'StoreCount' and 'average FRCS' for each tract\n",
        "        X_data = np.array([(tract['StoreCount'], tract['average FRCS']) for tract in data])\n",
        "\n",
        "        for k in range(2, 11):\n",
        "            print('k ', k, ' theta ', theta)\n",
        "            kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "            kmeans.fit(X_data)\n",
        "            # Store WCSS and silhouette score for each k and theta\n",
        "            wcss_df.at[k, theta] = kmeans.inertia_\n",
        "            silhouette_df.at[k, theta] = silhouette_score(X_data, kmeans.labels_)\n",
        "\n",
        "\n",
        "        # Plotting the elbow method and silhouette scores with highlights for chosen k\n",
        "        plt.figure(figsize=(14, 7))\n",
        "\n",
        "\n",
        "    # Plotting 3D plots for WCSS and silhouette scores\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    # WCSS 3D Plot\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    X, Y = np.meshgrid(theta_values, range(2, 11))\n",
        "    Z = wcss_df.values\n",
        "    ax1.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
        "    ax1.set_title('WCSS across Theta and k')\n",
        "    ax1.set_xlabel('Theta')\n",
        "    ax1.set_ylabel('k')\n",
        "    ax1.set_zlabel('WCSS')\n",
        "\n",
        "    # Silhouette Scores 3D Plot\n",
        "    ax2 = fig.add_subplot(122, projection='3d')\n",
        "    Z = silhouette_df.values\n",
        "    ax2.plot_surface(X, Y, Z, cmap='plasma', edgecolor='none')\n",
        "    ax2.set_title('Silhouette Scores across Theta and k')\n",
        "    ax2.set_xlabel('Theta')\n",
        "    ax2.set_ylabel('k')\n",
        "    ax2.set_zlabel('Silhouette Score')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    X, Y = np.meshgrid(wcss_df.columns, wcss_df.index)  # Theta values and k values\n",
        "    Z_wcss = wcss_df.values\n",
        "    Z_silhouette = silhouette_df.values\n",
        "\n",
        "    # Plot for WCSS\n",
        "    fig_wcss = go.Figure(data=[go.Surface(z=Z_wcss, x=X, y=Y)])\n",
        "    fig_wcss.update_layout(title='WCSS across Theta and k', autosize=False,\n",
        "                          scene=dict(\n",
        "                              xaxis_title='Theta',\n",
        "                              yaxis_title='k',\n",
        "                              zaxis_title='WCSS'),\n",
        "                          width=700, height=700,\n",
        "                          margin=dict(l=65, r=50, b=65, t=90))\n",
        "    fig_wcss.show()\n",
        "\n",
        "    # Plot for Silhouette Scores\n",
        "    fig_silhouette = go.Figure(data=[go.Surface(z=Z_silhouette, x=X, y=Y, colorscale='Viridis')])\n",
        "    fig_silhouette.update_layout(title='Silhouette Scores across Theta and k', autosize=False,\n",
        "                                scene=dict(\n",
        "                                    xaxis_title='Theta',\n",
        "                                    yaxis_title='k',\n",
        "                                    zaxis_title='Silhouette Score'),\n",
        "                                width=700, height=700,\n",
        "                                margin=dict(l=65, r=50, b=65, t=90))\n",
        "    fig_silhouette.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_census_data_to_tract(tract):\n",
        "    # Base URL for the Census API\n",
        "    base_url = \"https://api.census.gov/data\"\n",
        "\n",
        "    # Set year and dataset\n",
        "    year = \"2022\"\n",
        "    dataset = \"acs/acs5\"\n",
        "\n",
        "    # Variables for API request\n",
        "    variables = \"B19013_001E,B17001_002E,B22010_001E,B19083_001E,B23025_005E,B15003_002E,B16010_041E,B15003_022E,B25077_001E,B08301_010E,B25044_003E,B01003_001E,B02001_003E,B03002_012E,B02001_005E,B02001_004E,B02001_006E,B01001_001E\"\n",
        "\n",
        "\n",
        "    variable_labels = [\n",
        "        \"MedianIncome\", \"PovertyRate\", \"HHWithSNAP\", \"Inequality\", \"Unemployment\",\n",
        "        \"BelowHighSchool\", \"CollegeNoDegree\", \"BachelorsOrMore\", \"PropertyValue\",\n",
        "        \"PublicTransport\", \"NoVehicle\", \"Population\", \"Black\", \"Hispanic\",\n",
        "        \"Asian\", \"NativeAmerican\", \"PacificIslander\",\n",
        "    ]\n",
        "\n",
        "    median_values = {\n",
        "        \"MedianIncome\": 88921.50,\n",
        "        \"PovertyRate\": 0.07,\n",
        "        \"HHWithSNAP\": 1657.50,\n",
        "        \"Inequality\": 0.41,\n",
        "        \"Unemployment\": 0.03,\n",
        "        \"BelowHighSchool\": 0.00,\n",
        "        \"CollegeNoDegree\": 0.26,\n",
        "        \"BachelorsOrMore\": 0.18,\n",
        "        \"PropertyValue\": 292050.00,\n",
        "        \"PublicTransport\": 0.02,\n",
        "        \"NoVehicle\": 0.01,\n",
        "        \"Population\": 4581.50,\n",
        "        \"Black\": 0.12,\n",
        "        \"Hispanic\": 0.10,\n",
        "        \"Asian\": 0.05,\n",
        "        \"NativeAmerican\": 0.00,\n",
        "        \"PacificIslander\": 0.00\n",
        "    }\n",
        "\n",
        "    # Assuming tract_code is correctly set\n",
        "    print(tract[\"CensusTractID\"])\n",
        "    state_code = tract[\"CensusTractID\"][:2]\n",
        "    county_code = tract[\"CensusTractID\"][2:5]\n",
        "    tract_code = tract[\"CensusTractID\"][5:]\n",
        "\n",
        "\n",
        "    url = f\"{base_url}/{year}/{dataset}?get={variables}&for=tract:{tract_code}&in=state:{state_code}+county:{county_code}\"\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()[1]  # First element is header, second is data\n",
        "        total_population = float(data[variable_labels.index(\"Population\")])\n",
        "\n",
        "        for i, label in enumerate(variable_labels):\n",
        "            try:\n",
        "                value = float(data[i])\n",
        "                # Check if the label is among those that need to be converted to percentages\n",
        "                if label in [\"Unemployment\", \"PovertyRate\", \"BelowHighSchool\", \"CollegeNoDegree\", \"BachelorsOrMore\", \"PublicTransport\", \"NoVehicle\", \"Black\", \"Hispanic\", \"Asian\", \"NativeAmerican\", \"PacificIslander\"]:\n",
        "                    # Calculate percentage and constrain to non-negative values\n",
        "                    percent_value = max(0, (value / total_population))\n",
        "                    # Replace with median if original value is less than or equal to zero\n",
        "                    tract[label] = round(percent_value, 4) if value > 0 else median_values[label]\n",
        "                    if value < 0:\n",
        "                        print('negative ', label, ' averted')\n",
        "                else:\n",
        "                    # For non-percentage values, replace with median if value is less than or equal to zero\n",
        "                    tract[label] = value if value > 0 else median_values[label]\n",
        "            except ValueError:\n",
        "                tract[label] = median_values[label]\n",
        "            except ZeroDivisionError:\n",
        "                tract[label] = median_values[label]\n",
        "    # Print each label and its value for verification\n",
        "    for label in variable_labels:\n",
        "        print(label, tract[label])\n",
        "\n",
        "    # Return the updated tract dictionary\n",
        "    return tract\n",
        "\n",
        "# Function to add census data for all tracts\n",
        "def add_all_census_data(data, stores_data):\n",
        "    theta_values=np.linspace(0, 1, 21)\n",
        "    tracts_data = []\n",
        "    for index, tract in enumerate(data):\n",
        "\n",
        "        print(f'Processing tract {index + 1} of {len(data)}')\n",
        "        add_census_data_to_tract(tract)\n",
        "\n",
        "    return \"done\"\n",
        "\n",
        "\n",
        "# Prepares features for K-Means clustering\n",
        "def prepare_data_for_clustering(data):\n",
        "    # Initialize lists to hold the features\n",
        "    features = []\n",
        "\n",
        "    for tract in data:\n",
        "        store_count = tract['StoreCount']\n",
        "        average_frcs = tract['average FRCS']\n",
        "\n",
        "        tract_features = [store_count, average_frcs]\n",
        "        features.append(tract_features)\n",
        "\n",
        "    # Convert features to NumPy arrays\n",
        "    features_array = np.array(features)\n",
        "\n",
        "    return features_array\n",
        "\n",
        "\n",
        "# Preparing features for Random Forest classification\n",
        "def prepare_data_for_rf(data):\n",
        "    features = []\n",
        "\n",
        "    for tract in data:\n",
        "        tract_features = [\n",
        "            float(tract.get('MedianIncome')),\n",
        "            float(tract.get('PovertyRate')),\n",
        "            float(tract.get('HHWithSNAP')),\n",
        "            float(tract.get('Inequality')),\n",
        "            float(tract.get('Unemployment')),\n",
        "            float(tract.get('BelowHighSchool')),\n",
        "            float(tract.get('CollegeNoDegree')),\n",
        "            float(tract.get('BachelorsOrMore')),\n",
        "            float(tract.get('PropertyValue')),\n",
        "            float(tract.get('PublicTransport')),\n",
        "            float(tract.get('NoVehicle')),\n",
        "            float(tract.get('Population')),\n",
        "            float(tract.get('Black')),\n",
        "            float(tract.get('Hispanic')),\n",
        "            float(tract.get('Asian')),\n",
        "            float(tract.get('NativeAmerican')),\n",
        "            float(tract.get('PacificIslander'))\n",
        "        ]\n",
        "        features.append(tract_features)\n",
        "\n",
        "\n",
        "    # Convert features list to NumPy array\n",
        "    features_array = np.array(features)\n",
        "\n",
        "    return features_array\n",
        "\n",
        "def plot_k_theta_accuracy(data):\n",
        "    theta_values = np.linspace(0, 1, 21)\n",
        "    k_values = range(2, 12)\n",
        "    accuracies = np.zeros((len(theta_values), len(k_values)))\n",
        "\n",
        "    for i, theta in enumerate(theta_values):\n",
        "        print('Processing for theta:', theta)\n",
        "        update_json_with_frcs(theta, data)\n",
        "        features = prepare_data_for_clustering(data)\n",
        "\n",
        "        for j, k in enumerate(k_values):\n",
        "              kmeans = KMeans(n_clusters=k, random_state=42).fit(features)\n",
        "              labels = kmeans.labels_\n",
        "\n",
        "              features_rf = prepare_data_for_rf(data)\n",
        "\n",
        "              # Check if any class has fewer members than n_splits\n",
        "              if np.any(np.bincount(labels) < 5):  # Using 5 as the n_splits for StratifiedKFold\n",
        "                  print(\"Falling back to simple train-test split due to small class size.\")\n",
        "                  X_train, X_test, y_train, y_test = train_test_split(features_rf, labels, test_size=0.3, random_state=42)\n",
        "                  rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                  rf.fit(X_train, y_train)\n",
        "                  y_pred = rf.predict(X_test)\n",
        "                  accuracy = accuracy_score(y_test, y_pred)\n",
        "              else:\n",
        "                  # Continue With StratifiedKFold and SMOTE if applicable\n",
        "                  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "                  accuracies_skf = []\n",
        "\n",
        "                  for train_index, test_index in skf.split(features_rf, labels):\n",
        "                      X_train, X_test = features_rf[train_index], features_rf[test_index]\n",
        "                      y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "                      # Dynamically set SMOTE's k_neighbors parameter\n",
        "                      #smote_k_neighbors = min(len(np.unique(y_train)) - 1, np.bincount(y_train).min() - 1, 5)\n",
        "                      smote_k_neighbors = min([y_train.tolist().count(class_label) - 1 for class_label in np.unique(y_train)])\n",
        "                      smote_k_neighbors = max(1, smote_k_neighbors)  # Ensure k_neighbors is at least 1\n",
        "\n",
        "                      smote = SMOTE(random_state=42, k_neighbors=smote_k_neighbors)\n",
        "\n",
        "                      rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                      pipeline = make_pipeline(smote, rf)\n",
        "\n",
        "                      pipeline.fit(X_train, y_train)\n",
        "                      y_pred = pipeline.predict(X_test)\n",
        "\n",
        "                      accuracies_skf.append(accuracy_score(y_test, y_pred))\n",
        "                  accuracy = np.mean(accuracies_skf)\n",
        "              accuracies[i, j] = accuracy\n",
        "              print(f\"Average Accuracy for theta {theta:.2f} and k {k}: {accuracy:.4f}\")\n",
        "\n",
        "    # Plotting code\n",
        "    k_values_list = list(k_values)\n",
        "    T, K = np.meshgrid(np.array(theta_values), np.array(k_values_list))\n",
        "    Z = accuracies.T  # Transpose for Plotly\n",
        "\n",
        "    indices = [int(i * (len(theta_values) - 1)) for i in [0, 0.25, 0.5, 0.75, 1]]\n",
        "    ticktext = [f\"{theta_values[i]:.2f}\" for i in indices]\n",
        "\n",
        "\n",
        "    fig = go.Figure(data=[go.Surface(z=Z, x=T, y=K, colorscale='Viridis')])\n",
        "    fig.update_layout(\n",
        "        title='Accuracy across Theta and k',\n",
        "        scene=dict(\n",
        "            xaxis=dict(\n",
        "                title='Theta',\n",
        "                tickvals=indices,\n",
        "                ticktext=ticktext\n",
        "            ),\n",
        "            yaxis=dict(\n",
        "                title='k',\n",
        "                tickvals=list(range(len(k_values_list))),\n",
        "                ticktext=[str(k) for k in k_values_list]\n",
        "            ),\n",
        "            zaxis=dict(title='Accuracy'),\n",
        "        ),\n",
        "        autosize=True\n",
        "    )\n",
        "    fig.show()\n",
        "    return Z\n",
        "\n",
        "\n",
        "\n",
        "# Average FRCS of grocery and convenience stores\n",
        "def f_grocery(theta):\n",
        "    return np.interp(theta, avg_frcs_grocery.index, avg_frcs_grocery.values)\n",
        "def f_convenience(theta):\n",
        "    return np.interp(theta, avg_frcs_convenience.index, avg_frcs_convenience.values)\n",
        "\n",
        "# Find intersection\n",
        "def find_intersection(theta):\n",
        "    return f_grocery(theta) - f_convenience(theta)\n",
        "\n",
        "\n",
        "\n",
        "df_stores = pd.DataFrame(stores_data)\n",
        "\n",
        "# Calculate FRCS for each store at each theta\n",
        "theta_values = np.linspace(0, 1, num=101)\n",
        "\n",
        "frcs_values = pd.DataFrame(index=df_stores.index, columns=theta_values)\n",
        "\n",
        "# Calculate the FRCS for each store and each theta value\n",
        "for theta in theta_values:\n",
        "    frcs_values[theta] = df_stores.apply(calculate_frcs, axis=1, args=(theta,))\n",
        "\n",
        "# Add the store names and categories to the frcs_values DataFrame for reference\n",
        "frcs_values['Store'] = df_stores['Store']\n",
        "frcs_values['Category'] = df_stores['Category']\n",
        "\n",
        "# Split the DataFrame into categories\n",
        "grocery_stores = frcs_values[frcs_values['Category'] == 'grocery_or_supermarket']\n",
        "convenience_stores = frcs_values[frcs_values['Category'] == 'convenience_store']\n",
        "fast_food_stores = frcs_values[frcs_values['Category'] == 'meal_takeaway']\n",
        "\n",
        "# Calculate the average FRCS for each category across all theta values\n",
        "avg_frcs_grocery = grocery_stores[theta_values].mean()\n",
        "avg_frcs_convenience = convenience_stores[theta_values].mean()\n",
        "avg_frcs_fast_food = fast_food_stores[theta_values].mean()\n",
        "\n",
        "# Use fsolve to find the intersection point, starting the search at theta=0.5\n",
        "theta_intersection = fsolve(find_intersection, 0.5)[0]\n",
        "frcs_intersection = f_grocery(theta_intersection)\n",
        "\n",
        "\n",
        "# Print the intersection point\n",
        "print(f\"The lines intersect at theta = {theta_intersection:.2f} with an average FRCS of {frcs_intersection:.2f}\")\n",
        "\n",
        "\n",
        "# Plotting the intersection point\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot each category\n",
        "plt.plot(avg_frcs_grocery.index, avg_frcs_grocery.values, marker='o', linestyle='-', color='black', label='Grocery Store')\n",
        "plt.plot(avg_frcs_convenience.index, avg_frcs_convenience.values, marker='x', linestyle='--', color='grey', label='Convenience Store')\n",
        "plt.plot(avg_frcs_fast_food.index, avg_frcs_fast_food.values, marker='^', linestyle='-.', color='darkgrey', label='Fast Food')\n",
        "\n",
        "# Mark the intersection point with a star\n",
        "plt.plot(theta_intersection, frcs_intersection, marker='*', markersize=10, color='red', label='Intersection')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Average FRCS Across Observed Retailers by Categories as a Function of Theta')\n",
        "plt.xlabel('Theta')\n",
        "plt.ylabel('Average FRCS')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('Imports Completed, Data & Functions Loaded')"
      ],
      "metadata": {
        "id": "ifY-CPIZZPYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create plots to analyze optimal K with WCSS and Silhouette Scores"
      ],
      "metadata": {
        "id": "UvJSdFI3a7IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_optimal_k_and_plot_3D(data, np.linspace(0, 1, 11))"
      ],
      "metadata": {
        "id": "sNKKCXPsa9j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add all Census Data to JSON"
      ],
      "metadata": {
        "id": "dkvf4_bGa-Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(add_all_census_data(data, stores_data))"
      ],
      "metadata": {
        "id": "WJqXa8gObCX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create plot to analyze K and $\\Theta$ for accuracy using RK/SMOTE/StratifiedKFold"
      ],
      "metadata": {
        "id": "QHuzEZ_BbCza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Util code to fix axis values for plot"
      ],
      "metadata": {
        "id": "ovHsYobAb08m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the plotting function\n",
        "Z = plot_k_theta_accuracy(data)\n",
        "\n",
        "# Fix plot\n",
        "theta_values = np.linspace(0, 1, 21)\n",
        "k_values = range(2, 12)\n",
        "k_values_list = list(k_values)\n",
        "T, K = np.meshgrid(range(len(theta_values)), range(len(k_values)))\n",
        "\n",
        "# Indices for specific theta values (0, 0.25, 0.5, 0.75, 1)\n",
        "indices = [int(i * (len(theta_values) - 1)) for i in [0, 0.25, 0.5, 0.75, 1]]\n",
        "ticktext = [f\"{theta_values[i]:.2f}\" for i in indices]\n",
        "\n",
        "fig = go.Figure(data=[go.Surface(z=Z, x=T, y=K, colorscale='Viridis')])\n",
        "fig.update_layout(\n",
        "    title='Accuracy across Theta and k',\n",
        "    scene=dict(\n",
        "        xaxis=dict(\n",
        "            title='Theta',\n",
        "            tickvals=indices,\n",
        "            ticktext=ticktext\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title='k',\n",
        "            tickvals=list(range(len(k_values))),\n",
        "            ticktext=[str(k) for k in k_values]\n",
        "        ),\n",
        "        zaxis=dict(title='Accuracy'),\n",
        "    ),\n",
        "    autosize=True\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "EwRvxwxDbKOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Value at $\\Theta=0.66$, $K=3$, Plot Cluster Plot"
      ],
      "metadata": {
        "id": "M1rChcvXb5cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_at_theta_k(data, theta):\n",
        "    update_json_with_frcs(theta, data)\n",
        "    features = prepare_data_for_clustering(data)\n",
        "\n",
        "    # Apply K-Means clustering with k=3\n",
        "    kmeans = KMeans(n_clusters=3, random_state=42).fit(features)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Define cluster labels\n",
        "    cluster_labels = {0: 'Food Swamp', 1: 'Food Oasis', 2: 'Food Desert'}  # Update as necessary\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['green', 'blue', 'orange']\n",
        "    for cluster in range(3):\n",
        "        clustered_df = df[labels == cluster]\n",
        "        # Use cluster_labels for legend\n",
        "        plt.scatter(clustered_df['StoreCount'], clustered_df['AverageFRCS'], color=colors[cluster], label=cluster_labels[cluster], alpha=0.6, edgecolor='black')\n",
        "\n",
        "    plt.title('K-Means Clustering of Census Tracts')\n",
        "    plt.xlabel('Store Count')\n",
        "    plt.ylabel('Average FRCS')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    features_rf = prepare_data_for_rf(data)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    accuracies = []\n",
        "    feature_importances_list = []\n",
        "\n",
        "    all_true_labels = []\n",
        "    all_predictions = []\n",
        "    feature_names = [\n",
        "        \"MedianIncome\", \"PovertyRate\", \"HHWithSNAP\", \"Inequality\", \"Unemployment\",\n",
        "        \"BelowHighSchool\", \"CollegeNoDegree\", \"BachelorsOrMore\", \"PropertyValue\",\n",
        "        \"PublicTransport\", \"NoVehicle\", \"Population\", \"Black\", \"Hispanic\",\n",
        "        \"Asian\", \"NativeAmerican\", \"PacificIslander\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    for train_index, test_index in skf.split(features_rf, labels):\n",
        "        X_train, X_test = features_rf[train_index], features_rf[test_index]\n",
        "        y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "        # Dynamically adjust SMOTE's k_neighbors parameter\n",
        "        smote_k_neighbors = min([y_train.tolist().count(class_label) - 1 for class_label in np.unique(y_train)])\n",
        "        smote_k_neighbors = max(1, smote_k_neighbors)  # Ensure k_neighbors is at least 1\n",
        "\n",
        "\n",
        "        smote = SMOTE(random_state=42, k_neighbors=smote_k_neighbors)\n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        pipeline = Pipeline([('smote', smote), ('rf', rf)])\n",
        "\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "        all_true_labels.extend(y_test)\n",
        "        all_predictions.extend(y_pred)\n",
        "\n",
        "        rf_feature_importances = pipeline.named_steps['rf'].feature_importances_\n",
        "        feature_importances_list.append(rf_feature_importances)\n",
        "\n",
        "    average_accuracy = np.mean(accuracies)\n",
        "    print(\"Average Accuracy:\", average_accuracy)\n",
        "    print(\"Accuracy per fold:\", accuracies)\n",
        "\n",
        "    average_feature_importances = np.mean(feature_importances_list, axis=0)\n",
        "    feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': average_feature_importances}).sort_values(by='Importance', ascending=False)\n",
        "    print(\"Average Feature Importances across folds:\\n\", feature_importances_df)\n",
        "\n",
        "    print(\"Classification Report for All Folds:\")\n",
        "    class_names = [\"Food Swamp\", \"Food Oasis\", \"Food Desert\"]\n",
        "    print(classification_report(all_true_labels, all_predictions, target_names=class_names))\n",
        "\n",
        "\n",
        "evaluate_model_at_theta_k(data, 0.66)\n"
      ],
      "metadata": {
        "id": "yHH7PUYTcBkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Feature Statistics and Summary Table"
      ],
      "metadata": {
        "id": "AqbpCQLpc2WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_feature_statistics(features_array, feature_names):\n",
        "    # Ensure features_array is a NumPy array\n",
        "    features_array = np.array(features_array)\n",
        "\n",
        "    # Iterate over each feature by index and name\n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        # Calculate mean and standard deviation for the current feature across all tracts\n",
        "        feature_mean = np.mean(features_array[:, i])\n",
        "        feature_std = np.std(features_array[:, i])\n",
        "        feature_min = np.min(features_array[:, i])\n",
        "        feature_max = np.max(features_array[:, i])\n",
        "        feature_median = np.median(features_array[:, i])\n",
        "\n",
        "        #  Min = {feature_min:.2f}, Max = {feature_max:.2f}, Median = {feature_median:.2f}\"\n",
        "        # Print the statistics\n",
        "        print(f\"{feature_name}: Mean = {feature_mean:.2f}, SD = {feature_std:.2f}, max = {feature_max:.2f}, min = {feature_min:.2f}\")\n",
        "\n",
        "# Example feature names\n",
        "feature_names = [\n",
        "    \"MedianIncome\", \"PovertyRate\", \"HHWithSNAP\", \"Inequality\",\n",
        "    \"Unemployment\", \"BelowHighSchool\", \"CollegeNoDegree\",\n",
        "    \"BachelorsOrMore\", \"PropertyValue\", \"PublicTransport\",\n",
        "    \"NoVehicle\", \"Population\", \"Black\", \"Hispanic\",\n",
        "    \"Asian\", \"NativeAmerican\", \"PacificIslander\"\n",
        "]\n",
        "\n",
        "print_feature_statistics(prepare_data_for_rf(data), feature_names)\n",
        "\n",
        "features_array = prepare_data_for_rf(data)\n",
        "features = prepare_data_for_clustering(data)\n",
        "\n",
        "# Apply K-Means clustering with k=3\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(features)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "\n",
        "# Example feature names\n",
        "feature_names = [\n",
        "    \"MedianIncome\", \"PovertyRate\", \"HHWithSNAP\", \"Inequality\",\n",
        "    \"Unemployment\", \"BelowHighSchool\", \"CollegeNoDegree\",\n",
        "    \"BachelorsOrMore\", \"PropertyValue\", \"PublicTransport\",\n",
        "    \"NoVehicle\", \"Population\", \"Black\", \"Hispanic\",\n",
        "    \"Asian\", \"NativeAmerican\", \"PacificIslander\"\n",
        "]\n",
        "\n",
        "def print_feature_statistics_by_classification(features_array, feature_names, labels):\n",
        "    df = pd.DataFrame(features_array, columns=feature_names)\n",
        "    df['Classification'] = labels\n",
        "\n",
        "    summary_table = pd.DataFrame()\n",
        "\n",
        "    for feature_name in feature_names:\n",
        "        summary = df.groupby('Classification')[feature_name].agg(['mean', 'std']).apply(lambda x: f\"{x['mean']:.2f} ({x['std']:.2f})\", axis=1)\n",
        "        summary_table[feature_name] = summary\n",
        "\n",
        "    return summary_table\n",
        "\n",
        "summary_table = print_feature_statistics_by_classification(features_array, feature_names, labels)\n",
        "print(summary_table)"
      ],
      "metadata": {
        "id": "hjqpaZiRc4Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Map of Mercer County with Labels for $k=3$,$\\Theta=0.66$"
      ],
      "metadata": {
        "id": "hQj9n9wedt9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Prepare DataFrame for clustering\n",
        "tract_info = []\n",
        "for item in data:\n",
        "    tract_info.append({\n",
        "        \"CensusTractID\": item[\"CensusTractID\"],\n",
        "        \"StoreCount\": item[\"StoreCount\"],\n",
        "        \"AverageFRCS\": item[\"average FRCS\"]  # Using get() to avoid KeyError if missing\n",
        "    })\n",
        "\n",
        "df_tracts = pd.DataFrame(tract_info)\n",
        "\n",
        "# Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "df_tracts['Cluster'] = kmeans.fit_predict(df_tracts[['StoreCount', 'AverageFRCS']])\n",
        "\n",
        "# Map CensusTractID to GEOID_x for merging\n",
        "df_tracts['GEOID_x'] = df_tracts['CensusTractID']\n",
        "\n",
        "# Merge the cluster information back to your GeoDataFrame\n",
        "merged_gdf = filtered_gdf.merge(df_tracts[['GEOID_x', 'Cluster']], on='GEOID_x')\n",
        "\n",
        "# Prep Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Define custom labels and colors for clusters\n",
        "cluster_labels = {0: 'Food Swamp', 1: 'Food Oasis', 2: 'Food Desert'}\n",
        "cluster_colors = ['green', 'blue', 'orange']\n",
        "\n",
        "# Plot each cluster with its color and label\n",
        "for cluster, color in zip(cluster_labels.keys(), cluster_colors):\n",
        "    merged_gdf.loc[merged_gdf['Cluster'] == cluster].plot(color=color, ax=ax, label=cluster_labels[cluster])\n",
        "\n",
        "# Create custom legend\n",
        "legend_patches = [mpatches.Patch(color=color, label=label) for color, label in zip(cluster_colors, cluster_labels.values())]\n",
        "ax.legend(handles=legend_patches, title=\"Classification\")\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3qTmJY4MdmBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}